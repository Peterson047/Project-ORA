<div align="center">
  <img src="ollama-nextjs-ui.gif">
</div>

<h1 align="center">
  ORA Model + Ollama LLM UI
</h1>

<div align="center">
  
![GitHub Repo stars](https://img.shields.io/github/stars/jakobhoeg/nextjs-ollama-llm-ui)
  
</div>

Comece rapidamente com Modelos de Linguagem Grande (LLMs) de forma **r√°pida**, **local** e at√© mesmo **offline**. Este projeto tem como objetivo ser a maneira mais f√°cil para voc√™ come√ßar com LLMs. Nenhuma configura√ß√£o tediosa e irritante √© necess√°ria!

# Funcionalidades ‚ú®

- **UI bonita e intuitiva:** Inspirada no ChatGPT, para melhorar a similaridade na experi√™ncia do usu√°rio.
- **Totalmente local:** Armazena conversas no armazenamento local para conveni√™ncia. Sem necessidade de executar um banco de dados.
- **Totalmente responsivo:** Use seu celular para conversar, com a mesma facilidade que no desktop.
- **Configura√ß√£o f√°cil:** Nenhuma configura√ß√£o tediosa e irritante √© necess√°ria. Basta clonar o reposit√≥rio e come√ßar!
- **Realce de sintaxe de c√≥digo:** Mensagens que incluem c√≥digo s√£o destacadas para f√°cil acesso.
- **Copie blocos de c√≥digo facilmente:** Copie facilmente o c√≥digo destacado com um clique.
- **Download/Pull & Delete de modelos:** Fa√ßa download e delete modelos diretamente pela interface.
- **Alternar entre modelos:** Troque entre modelos rapidamente com um clique.
- **Hist√≥rico de conversas:** As conversas s√£o salvas e facilmente acessadas.
- **Modo claro e escuro:** Alternar entre modo claro e escuro.

# Pr√©via

https://github.com/jakobhoeg/nextjs-ollama-llm-ui/assets/114422072/08eaed4f-9deb-4e1b-b87a-ba17d81b9a02

# Requisitos ‚öôÔ∏è

Para usar a interface web, estes requisitos devem ser atendidos:

1. Baixe [Ollama](https://ollama.com/download) e o tenha em execu√ß√£o. Ou execute-o em um cont√™iner Docker. Verifique a [documenta√ß√£o](https://github.com/ollama/ollama) para instru√ß√µes.
2. Node.js (18+) e npm s√£o necess√°rios. [Download](https://nodejs.org/en/download)

# Implemente o seu pr√≥prio no Vercel ou Netlify com um clique ‚ú®

[![Implementar com Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https%3A%2F%2Fgithub.com%2Fjakobhoeg%2Fnextjs-ollama-llm-ui&env=NEXT_PUBLIC_OLLAMA_URL&envDescription=Sua%20URL%20Ollama) [![Bot√£o de implementa√ß√£o no Netlify](https://www.netlify.com/img/deploy/button.svg)](https://app.netlify.com/start/deploy?repository=https://github.com/jakobhoeg/nextjs-ollama-llm-ui)

Voc√™ precisar√° definir sua vari√°vel de ambiente [OLLAMA_ORIGINS](https://github.com/ollama/ollama/blob/main/docs/faq.md) em sua m√°quina que est√° executando Ollama:


```
OLLAMA_ORIGINS="https://seu-app.vercel.app/"
```


# Instala√ß√£o üìñ

[![Status de empacotamento](https://repology.org/badge/vertical-allrepos/nextjs-ollama-llm-ui.svg?columns=3)](https://repology.org/project/nextjs-ollama-llm-ui/versions)

Use um pacote pr√©-constru√≠do de um dos gerenciadores de pacotes suportados para executar um ambiente local da interface web. Alternativamente, voc√™ pode instalar a partir do c√≥digo-fonte com as instru√ß√µes abaixo.

> [!NOTA]  
> Se seu frontend rodar em algo diferente de `http://localhost` ou `http://127.0.0.1`, voc√™ precisar√° definir OLLAMA_ORIGINS para a URL do seu frontend.
>
> Isso tamb√©m est√° declarado na [documenta√ß√£o](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-do-i-configure-ollama-server):
> 
> `Ollama permite solicita√ß√µes de origens cruzadas de 127.0.0.1 e 0.0.0.0 por padr√£o. Origens adicionais podem ser configuradas com OLLAMA_ORIGINS`

## Instalar a partir do c√≥digo-fonte

**1. Clone o reposit√≥rio para um diret√≥rio em seu PC via prompt de comando:**
````git clone https://github.com/jakobhoeg/nextjs-ollama-llm-ui```

```
git clone https://github.com/jakobhoeg/nextjs-ollama-llm-ui
```

**2. Open the folder:**

```
cd nextjs-ollama-llm-ui
```

**3. Rename the `.example.env` to `.env`:**

```
mv .example.env .env
```

**4. If your instance of Ollama is NOT running on the default ip-address and port, change the variable in the .env file to fit your usecase:**

```
NEXT_PUBLIC_OLLAMA_URL="http://localhost:11434"
```

**5. Install dependencies:**

```
npm install
```

**6. Start the development server:**

```
npm run dev
```

**5. Go to [localhost:3000](http://localhost:3000) and start chatting with your favourite model!**

# Upcoming features

This is a to-do list consisting of upcoming features.
- ‚úÖ Voice input support
- ‚úÖ Code syntax highlighting
- ‚¨úÔ∏è Ability to send an image in the prompt to utilize vision language models.
- ‚¨úÔ∏è Ability to regenerate responses
- ‚¨úÔ∏è Import and export chats

# Tech stack

[NextJS](https://nextjs.org/) - React Framework for the Web

[TailwindCSS](https://tailwindcss.com/) - Utility-first CSS framework

[shadcn-ui](https://ui.shadcn.com/) - UI component built using Radix UI and Tailwind CSS

[shadcn-chat](https://github.com/jakobhoeg/shadcn-chat) - Chat components for NextJS/React projects

[Framer Motion](https://www.framer.com/motion/) - Motion/animation library for React

[Lucide Icons](https://lucide.dev/) - Icon library

# Helpful links

[Medium Article](https://medium.com/@bartek.lewicz/launch-your-own-chatgpt-clone-for-free-on-colab-shareable-and-online-in-less-than-10-minutes-da19e44be5eb) - How to launch your own ChatGPT clone for free on Google Colab. By Bartek Lewicz.

[Lobehub mention](https://lobehub.com/blog/5-ollama-web-ui-recommendation#5-next-js-ollama-llm-ui) - Five Excellent Free Ollama WebUI Client Recommendations
